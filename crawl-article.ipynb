{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def fetch_sitemap(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Không thể tải sitemap: {e}\")\n",
    "        return None\n",
    "\n",
    "def save_to_csv(filename, data, fieldnames):\n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "def read_from_csv(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        return [row for row in reader]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu 2960 journal URLs vào journals.csv.\n"
     ]
    }
   ],
   "source": [
    "# 1. Crawl sitemap để lấy danh sách journal URLs\n",
    "sitemap_url = \"https://link.springer.com/sitemap-springer-journals.xml\"\n",
    "sitemap_content = fetch_sitemap(sitemap_url)\n",
    "\n",
    "def parse_sitemap(xml_content):\n",
    "    urls = []\n",
    "    try:\n",
    "        root = ET.fromstring(xml_content)\n",
    "        for loc in root.iter(\"{http://www.sitemaps.org/schemas/sitemap/0.9}loc\"):\n",
    "            urls.append(loc.text.strip())\n",
    "    except ET.ParseError as e:\n",
    "        print(f\"Lỗi phân tích sitemap: {e}\")\n",
    "    return urls\n",
    "\n",
    "if sitemap_content:\n",
    "    journal_urls = parse_sitemap(sitemap_content)\n",
    "    journal_csv = \"journals.csv\"\n",
    "    save_to_csv(journal_csv, [{\"journal_url\": url} for url in journal_urls], [\"journal_url\"])\n",
    "    print(f\"Đã lưu {len(journal_urls)} journal URLs vào {journal_csv}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Đọc danh sách journal từ file\n",
    "journal_csv = \"journals.csv\"\n",
    "journals = read_from_csv(journal_csv)\n",
    "\n",
    "def fetch_article_links(journal_url):\n",
    "    all_links = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        url = f\"{journal_url}/articles?filterOpenAccess=false&page={page}\"\n",
    "        print(f\"Đang crawl trang: {url}\")\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            articles = soup.find_all(\"h3\", {\"class\": \"app-card-open__heading\"})\n",
    "            if not articles:  # Dừng nếu không còn bài viết\n",
    "                break\n",
    "            for article in articles:\n",
    "                link = article.find(\"a\")[\"href\"] if article.find(\"a\") else None\n",
    "                if link:\n",
    "                    all_links.append(link)\n",
    "            page += 1\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Lỗi khi tải trang {url}: {e}\")\n",
    "            break\n",
    "    return all_links\n",
    "\n",
    "# Crawl các bài viết \n",
    "all_articles = []\n",
    "journals_to_crawl = journals[:]  \n",
    "for journal in journals_to_crawl:\n",
    "    articles = fetch_article_links(journal[\"journal_url\"])\n",
    "    for article in articles:\n",
    "        all_articles.append({\"article_url\": article, \"journal_url\": journal[\"journal_url\"]})\n",
    "\n",
    "# Lưu danh sách bài viết vào file CSV\n",
    "articles_csv = \"article_links.csv\"  \n",
    "save_to_csv(articles_csv, all_articles, [\"article_url\", \"journal_url\"])\n",
    "print(f\"Đã lưu {len(all_articles)} bài viết từ journal vào {articles_csv}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Đọc danh sách bài viết từ file\n",
    "articles_csv = \"article_links.csv\"\n",
    "articles = read_from_csv(articles_csv)\n",
    "\n",
    "def fetch_article_data(article_url):\n",
    "    try:\n",
    "        response = requests.get(article_url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        title = soup.find(\"h1\", {\"class\": \"c-article-title\"}).get_text(strip=True) if soup.find(\"h1\", {\"class\": \"c-article-title\"}) else \"N/A\"\n",
    "\n",
    "        abstract_section = soup.find(\"div\", {\"id\":\"Abs1-section\"})\n",
    "        abstract = abstract_section.find(\"p\").get_text(strip=True) if abstract_section else \"N/A\"\n",
    "\n",
    "        return {\n",
    "            \"url\": article_url,\n",
    "            \"title\": title,\n",
    "            \"abstract\": abstract\n",
    "        }\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Lỗi khi tải bài viết {article_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Crawl dữ liệu chi tiết của từng bài viết\n",
    "articles_data = []\n",
    "for article in articles:\n",
    "    article_data = fetch_article_data(article[\"article_url\"])\n",
    "    if article_data:\n",
    "        articles_data.append(article_data)\n",
    "\n",
    "# Lưu dữ liệu chi tiết vào file CSV\n",
    "articles_data_csv = \"articles_data.csv\"\n",
    "save_to_csv(articles_data_csv, articles_data, [\"url\", \"title\", \"abstract\"])\n",
    "print(f\"Đã lưu dữ liệu chi tiết của bài viết vào {articles_data_csv}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
