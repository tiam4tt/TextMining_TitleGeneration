{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1. C\u00e0i \u0111\u1eb7t c\u00e1c th\u01b0 vi\u1ec7n c\u1ea7n thi\u1ebft tr\u00ean Kaggle\n",
    "!pip install transformers torch pandas numpy rouge-score bert-score huggingface_hub\n",
    "\n",
    "# 2. \u0110\u0103ng nh\u1eadp v\u00e0o Hugging Face \u0111\u1ec3 t\u1ea3i m\u00f4 h\u00ecnh\n",
    "from huggingface_hub import login\n",
    "\n",
    "# D\u00e1n API Token c\u1ee7a b\u1ea1n (l\u1ea5y t\u1eeb Hugging Face Settings > Access Tokens)\n",
    "api_token = \"\"  # Thay b\u1eb1ng API Token c\u1ee7a b\u1ea1n\n",
    "login(api_token)\n",
    "\n",
    "# 3. T\u1ea3i m\u00f4 h\u00ecnh v\u00e0 tokenizer t\u1eeb Hugging Face\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, GenerationConfig\n",
    "\n",
    "repo_id = \"HTThuanHcmus/bart-finetuned-scientific\"  # Thay b\u1eb1ng username v\u00e0 t\u00ean repo c\u1ee7a b\u1ea1n\n",
    "tokenizer = BartTokenizer.from_pretrained(repo_id)\n",
    "model = BartForConditionalGeneration.from_pretrained(repo_id)\n",
    "\n",
    "# T\u1ea1o GenerationConfig th\u1ee7 c\u00f4ng \u0111\u1ec3 tr\u00e1nh l\u1ed7i\n",
    "generation_config = GenerationConfig(\n",
    "    early_stopping=True,\n",
    "    max_length=32,\n",
    "    num_beams=5\n",
    ")\n",
    "model.generation_config = generation_config\n",
    "\n",
    "# 4. H\u00e0m d\u1ef1 \u0111o\u00e1n ti\u00eau \u0111\u1ec1 t\u1eeb abstract b\u1eb1ng BART\n",
    "def predict_title_bart(abstract):\n",
    "    inputs = tokenizer(abstract, max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=32,\n",
    "        num_beams=5,\n",
    "        early_stopping=True,\n",
    "        decoder_start_token_id=tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.convert_tokens_to_ids('<s>')\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 5. T\u1ea3i d\u1eef li\u1ec7u test tr\u00ean Kaggle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score\n",
    "\n",
    "# Gi\u1ea3 s\u1eed d\u1eef li\u1ec7u c\u1ee7a b\u1ea1n n\u1eb1m trong /kaggle/input\n",
    "# File CSV ch\u1ee9a c\u00e1c c\u1ed9t: abstract, title (ti\u00eau \u0111\u1ec1 th\u1ef1c t\u1ebf), llm_title (ti\u00eau \u0111\u1ec1 t\u1eeb LLM)\n",
    "data_df = pd.read_csv('/kaggle/input/compare-bart-llm/random_500_samples_full.csv')  # Thay b\u1eb1ng \u0111\u01b0\u1eddng d\u1eabn th\u1ef1c t\u1ebf tr\u00ean Kaggle\n",
    "abstracts = data_df['abstract'].tolist()\n",
    "true_titles = data_df['title'].tolist()\n",
    "llm_titles = data_df['title-llm'].tolist()\n",
    "\n",
    "# 6. D\u1ef1 \u0111o\u00e1n ti\u00eau \u0111\u1ec1 b\u1eb1ng BART\n",
    "bart_titles = [predict_title_bart(abs) for abs in abstracts]\n",
    "\n",
    "# 7. T\u00ednh ROUGE v\u00e0 BERTScore cho c\u1ea3 hai t\u1eadp ti\u00eau \u0111\u1ec1\n",
    "# ROUGE\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL', 'rougeLsum'], use_stemmer=True)\n",
    "\n",
    "# ROUGE cho LLM\n",
    "llm_rouge1_scores = []\n",
    "llm_rouge2_scores = []\n",
    "llm_rougeL_scores = []\n",
    "llm_rougeLsum_scores = []\n",
    "\n",
    "for pred, ref in zip(llm_titles, true_titles):\n",
    "    scores = scorer.score(ref, pred)\n",
    "    llm_rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "    llm_rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "    llm_rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "    llm_rougeLsum_scores.append(scores['rougeLsum'].fmeasure)\n",
    "\n",
    "# ROUGE cho BART\n",
    "bart_rouge1_scores = []\n",
    "bart_rouge2_scores = []\n",
    "bart_rougeL_scores = []\n",
    "bart_rougeLsum_scores = []\n",
    "\n",
    "for pred, ref in zip(bart_titles, true_titles):\n",
    "    scores = scorer.score(ref, pred)\n",
    "    bart_rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "    bart_rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "    bart_rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "    bart_rougeLsum_scores.append(scores['rougeLsum'].fmeasure)\n",
    "\n",
    "# BERTScore cho LLM\n",
    "llm_P, llm_R, llm_F1 = score(llm_titles, true_titles, lang=\"en\", verbose=True)\n",
    "llm_bert_precision_scores = llm_P.tolist()\n",
    "llm_bert_recall_scores = llm_R.tolist()\n",
    "llm_bert_f1_scores = llm_F1.tolist()\n",
    "\n",
    "# BERTScore cho BART\n",
    "bart_P, bart_R, bart_F1 = score(bart_titles, true_titles, lang=\"en\", verbose=True)\n",
    "bart_bert_precision_scores = bart_P.tolist()\n",
    "bart_bert_recall_scores = bart_R.tolist()\n",
    "bart_bert_f1_scores = bart_F1.tolist()\n",
    "\n",
    "# 8. So s\u00e1nh trung b\u00ecnh ROUGE v\u00e0 BERTScore\n",
    "print(\"So s\u00e1nh trung b\u00ecnh ROUGE v\u00e0 BERTScore:\\n\")\n",
    "\n",
    "print(\"LLM Titles:\")\n",
    "print(f\"ROUGE-1: {np.mean(llm_rouge1_scores):.4f}\")\n",
    "print(f\"ROUGE-2: {np.mean(llm_rouge2_scores):.4f}\")\n",
    "print(f\"ROUGE-L: {np.mean(llm_rougeL_scores):.4f}\")\n",
    "print(f\"ROUGE-Lsum: {np.mean(llm_rougeLsum_scores):.4f}\")\n",
    "print(f\"BERTScore Precision: {np.mean(llm_bert_precision_scores):.4f}\")\n",
    "print(f\"BERTScore Recall: {np.mean(llm_bert_recall_scores):.4f}\")\n",
    "print(f\"BERTScore F1: {np.mean(llm_bert_f1_scores):.4f}\\n\")\n",
    "\n",
    "print(\"BART Fine-tuned Titles:\")\n",
    "print(f\"ROUGE-1: {np.mean(bart_rouge1_scores):.4f}\")\n",
    "print(f\"ROUGE-2: {np.mean(bart_rouge2_scores):.4f}\")\n",
    "print(f\"ROUGE-L: {np.mean(bart_rougeL_scores):.4f}\")\n",
    "print(f\"ROUGE-Lsum: {np.mean(bart_rougeLsum_scores):.4f}\")\n",
    "print(f\"BERTScore Precision: {np.mean(bart_bert_precision_scores):.4f}\")\n",
    "print(f\"BERTScore Recall: {np.mean(bart_bert_recall_scores):.4f}\")\n",
    "print(f\"BERTScore F1: {np.mean(bart_bert_f1_scores):.4f}\\n\")\n",
    "\n",
    "# 9. Ph\u00e2n t\u00edch chi ti\u1ebft: S\u1ed1 l\u01b0\u1ee3ng ti\u00eau \u0111\u1ec1 m\u00e0 BART v\u01b0\u1ee3t tr\u1ed9i h\u01a1n LLM v\u00e0 ng\u01b0\u1ee3c l\u1ea1i\n",
    "bart_better_rouge1 = sum(1 for llm_score, bart_score in zip(llm_rouge1_scores, bart_rouge1_scores) if bart_score > llm_score)\n",
    "llm_better_rouge1 = sum(1 for llm_score, bart_score in zip(llm_rouge1_scores, bart_rouge1_scores) if llm_score > bart_score)\n",
    "\n",
    "bart_better_bertscore = sum(1 for llm_score, bart_score in zip(llm_bert_f1_scores, bart_bert_f1_scores) if bart_score > llm_score)\n",
    "llm_better_bertscore = sum(1 for llm_score, bart_score in zip(llm_bert_f1_scores, bart_bert_f1_scores) if llm_score > bart_score)\n",
    "\n",
    "print(\"Ph\u00e2n t\u00edch chi ti\u1ebft:\")\n",
    "print(f\"S\u1ed1 ti\u00eau \u0111\u1ec1 m\u00e0 BART c\u00f3 ROUGE-1 cao h\u01a1n LLM: {bart_better_rouge1}\")\n",
    "print(f\"S\u1ed1 ti\u00eau \u0111\u1ec1 m\u00e0 LLM c\u00f3 ROUGE-1 cao h\u01a1n BART: {llm_better_rouge1}\")\n",
    "print(f\"S\u1ed1 ti\u00eau \u0111\u1ec1 m\u00e0 BART c\u00f3 BERTScore F1 cao h\u01a1n LLM: {bart_better_bertscore}\")\n",
    "print(f\"S\u1ed1 ti\u00eau \u0111\u1ec1 m\u00e0 LLM c\u00f3 BERTScore F1 cao h\u01a1n BART: {llm_better_bertscore}\\n\")\n",
    "\n",
    "# 10. L\u01b0u k\u1ebft qu\u1ea3 chi ti\u1ebft v\u00e0o file CSV\n",
    "results_df = pd.DataFrame({\n",
    "    'abstract': abstracts,\n",
    "    'true_title': true_titles,\n",
    "    'llm_title': llm_titles,\n",
    "    'bart_title': bart_titles,\n",
    "    'llm_rouge1': llm_rouge1_scores,\n",
    "    'llm_rouge2': llm_rouge2_scores,\n",
    "    'llm_rougeL': llm_rougeL_scores,\n",
    "    'llm_rougeLsum': llm_rougeLsum_scores,\n",
    "    'llm_bertscore_precision': llm_bert_precision_scores,\n",
    "    'llm_bertscore_recall': llm_bert_recall_scores,\n",
    "    'llm_bertscore_f1': llm_bert_f1_scores,\n",
    "    'bart_rouge1': bart_rouge1_scores,\n",
    "    'bart_rouge2': bart_rouge2_scores,\n",
    "    'bart_rougeL': bart_rougeL_scores,\n",
    "    'bart_rougeLsum': bart_rougeLsum_scores,\n",
    "    'bart_bertscore_precision': bart_bert_precision_scores,\n",
    "    'bart_bertscore_recall': bart_bert_recall_scores,\n",
    "    'bart_bertscore_f1': bart_bert_f1_scores\n",
    "})\n",
    "results_df.to_csv('/kaggle/working/comparison_results.csv', index=False)\n",
    "print(\"\u0110\u00e3 l\u01b0u k\u1ebft qu\u1ea3 chi ti\u1ebft v\u00e0o '/kaggle/working/comparison_results.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7012787,
     "sourceId": 11227731,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}