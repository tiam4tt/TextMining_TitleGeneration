{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0a9a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -q -U google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9647c4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from google.genai.types import CreateBatchJobConfig, JobState, HttpOptions\n",
    "\n",
    "client = genai.Client(http_options=HttpOptions(api_version=\"v1\"),\n",
    "                      vertexai=True,\n",
    "                      project=\"gen-lang-client-0712587895\",\n",
    "                      location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1c0c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = client.models.list()\n",
    "# for model in models:\n",
    "#     model = model.__dict__\n",
    "#     print(model['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60db2beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# df = pd.read_csv(\"./data/test.csv\").reset_index(drop=True)\n",
    "# batches = np.array_split(df[\"abstract\"], 5)\n",
    "\n",
    "# instruction = \"Write a consise, formal and structured title for this scientific research work, return ONLY the title:\\n\"\n",
    "\n",
    "# for i, batch in enumerate(batches):\n",
    "#     with open(f\"./batches/gemini_batch_input_{i+1}.jsonl\", \"w\") as f:\n",
    "#         for line in batch:\n",
    "#             entry = {\n",
    "#                 \"request\":\n",
    "#                     {\"contents\": [\n",
    "#                             {\"role\": \"user\",\n",
    "#                             \"parts\": [\n",
    "#                                 {\"text\": instruction + line}\n",
    "#                                 ]\n",
    "#                             }\n",
    "#                         ]\n",
    "#                     }\n",
    "#                 }\n",
    "#             f.write(json.dumps(entry) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc3deb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud projects add-iam-policy-binding gen-lang-client-0712587895 \\\n",
    "  --member=\"serviceAccount:gemini-api@gen-lang-client-0712587895.iam.gserviceaccount.com\" \\\n",
    "  --role=\"roles/aiplatform.user\"\n",
    "\n",
    "# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n",
    "# with appropriate values for your project.\n",
    "!export GOOGLE_CLOUD_PROJECT=gen-lang-client-0712587895\n",
    "!export GOOGLE_CLOUD_LOCATION=us-central1\n",
    "!export GOOGLE_GENAI_USE_VERTEXAI=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e0780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_uri = \"gs://gemini_batch_input/output\"\n",
    "batch_jobs = []\n",
    "\n",
    "\n",
    "for i in range(1,6):\n",
    "    # See the documentation: https://googleapis.github.io/python-genai/genai.html#genai.batches.Batches.create\n",
    "    job = client.batches.create(\n",
    "        model=\"gemini-2.0-flash-001\",\n",
    "        # Source link: https://storage.cloud.google.com/cloud-samples-data/batch/prompt_for_batch_gemini_predict.jsonl\n",
    "        src=f\"gs://gemini_batch_input/gemini_batch_input_{i}.jsonl\",\n",
    "        config=CreateBatchJobConfig(dest=output_uri),\n",
    "    )\n",
    "    batch_jobs.append(job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10b8ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./batches/gemini_batch_jobs.txt\", \"w\") as f:\n",
    "    for bj in batch_jobs:\n",
    "        print(bj)\n",
    "        f.write(bj + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04933a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='projects/32693730648/locations/us-central1/batchPredictionJobs/2667732022952198144' display_name='genai_batch_job_20250417232732_a08aa' state=<JobState.JOB_STATE_SUCCEEDED: 'JOB_STATE_SUCCEEDED'> error=None create_time=datetime.datetime(2025, 4, 17, 16, 27, 33, 673067, tzinfo=TzInfo(UTC)) start_time=datetime.datetime(2025, 4, 17, 16, 52, 59, 246125, tzinfo=TzInfo(UTC)) end_time=datetime.datetime(2025, 4, 17, 16, 57, 27, 279260, tzinfo=TzInfo(UTC)) update_time=datetime.datetime(2025, 4, 17, 16, 57, 27, 279260, tzinfo=TzInfo(UTC)) model='publishers/google/models/gemini-2.0-flash-001' src=BatchJobSource(format='jsonl', gcs_uri=['gs://gemini_batch_input/gemini_batch_input_5.jsonl'], bigquery_uri=None) dest=BatchJobDestination(format='jsonl', gcs_uri='gs://gemini_batch_input/output', bigquery_uri=None)\n",
      "name='projects/32693730648/locations/us-central1/batchPredictionJobs/181745028643684352' display_name='genai_batch_job_20250417232732_c3721' state=<JobState.JOB_STATE_SUCCEEDED: 'JOB_STATE_SUCCEEDED'> error=None create_time=datetime.datetime(2025, 4, 17, 16, 27, 32, 748356, tzinfo=TzInfo(UTC)) start_time=datetime.datetime(2025, 4, 17, 16, 49, 2, 12861, tzinfo=TzInfo(UTC)) end_time=datetime.datetime(2025, 4, 17, 16, 52, 58, 912648, tzinfo=TzInfo(UTC)) update_time=datetime.datetime(2025, 4, 17, 16, 52, 58, 912648, tzinfo=TzInfo(UTC)) model='publishers/google/models/gemini-2.0-flash-001' src=BatchJobSource(format='jsonl', gcs_uri=['gs://gemini_batch_input/gemini_batch_input_4.jsonl'], bigquery_uri=None) dest=BatchJobDestination(format='jsonl', gcs_uri='gs://gemini_batch_input/output', bigquery_uri=None)\n",
      "name='projects/32693730648/locations/us-central1/batchPredictionJobs/6822584154178191360' display_name='genai_batch_job_20250417232731_7b609' state=<JobState.JOB_STATE_SUCCEEDED: 'JOB_STATE_SUCCEEDED'> error=None create_time=datetime.datetime(2025, 4, 17, 16, 27, 32, 399053, tzinfo=TzInfo(UTC)) start_time=datetime.datetime(2025, 4, 17, 16, 44, 4, 483380, tzinfo=TzInfo(UTC)) end_time=datetime.datetime(2025, 4, 17, 16, 49, 1, 899898, tzinfo=TzInfo(UTC)) update_time=datetime.datetime(2025, 4, 17, 16, 49, 1, 899898, tzinfo=TzInfo(UTC)) model='publishers/google/models/gemini-2.0-flash-001' src=BatchJobSource(format='jsonl', gcs_uri=['gs://gemini_batch_input/gemini_batch_input_3.jsonl'], bigquery_uri=None) dest=BatchJobDestination(format='jsonl', gcs_uri='gs://gemini_batch_input/output', bigquery_uri=None)\n",
      "name='projects/32693730648/locations/us-central1/batchPredictionJobs/1479907621233229824' display_name='genai_batch_job_20250417232731_a98ed' state=<JobState.JOB_STATE_SUCCEEDED: 'JOB_STATE_SUCCEEDED'> error=None create_time=datetime.datetime(2025, 4, 17, 16, 27, 31, 457020, tzinfo=TzInfo(UTC)) start_time=datetime.datetime(2025, 4, 17, 16, 37, 36, 328832, tzinfo=TzInfo(UTC)) end_time=datetime.datetime(2025, 4, 17, 16, 44, 4, 403742, tzinfo=TzInfo(UTC)) update_time=datetime.datetime(2025, 4, 17, 16, 44, 4, 403742, tzinfo=TzInfo(UTC)) model='publishers/google/models/gemini-2.0-flash-001' src=BatchJobSource(format='jsonl', gcs_uri=['gs://gemini_batch_input/gemini_batch_input_2.jsonl'], bigquery_uri=None) dest=BatchJobDestination(format='jsonl', gcs_uri='gs://gemini_batch_input/output', bigquery_uri=None)\n",
      "name='projects/32693730648/locations/us-central1/batchPredictionJobs/3364664065287782400' display_name='genai_batch_job_20250417232728_e72b5' state=<JobState.JOB_STATE_SUCCEEDED: 'JOB_STATE_SUCCEEDED'> error=None create_time=datetime.datetime(2025, 4, 17, 16, 27, 31, 121431, tzinfo=TzInfo(UTC)) start_time=datetime.datetime(2025, 4, 17, 16, 30, 54, 747895, tzinfo=TzInfo(UTC)) end_time=datetime.datetime(2025, 4, 17, 16, 37, 36, 227046, tzinfo=TzInfo(UTC)) update_time=datetime.datetime(2025, 4, 17, 16, 37, 36, 227046, tzinfo=TzInfo(UTC)) model='publishers/google/models/gemini-2.0-flash-001' src=BatchJobSource(format='jsonl', gcs_uri=['gs://gemini_batch_input/gemini_batch_input_1.jsonl'], bigquery_uri=None) dest=BatchJobDestination(format='jsonl', gcs_uri='gs://gemini_batch_input/output', bigquery_uri=None)\n"
     ]
    }
   ],
   "source": [
    "for job in client.batches.list(config=types.ListBatchJobsConfig(page_size=10)):\n",
    "    print(job)\n",
    "# for job_id in batch_jobs:\n",
    "#     job = client.batches.get(name=job_id)\n",
    "#     print(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cc4de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the documentation: https://googleapis.github.io/python-genai/genai.html#genai.types.BatchJob\n",
    "completed_states = {\n",
    "    JobState.JOB_STATE_SUCCEEDED,\n",
    "    JobState.JOB_STATE_FAILED,\n",
    "    JobState.JOB_STATE_CANCELLED,\n",
    "    JobState.JOB_STATE_PAUSED,\n",
    "}\n",
    "\n",
    "for job_id in batch_jobs:\n",
    "    job = client.batches.get(name=job_id)\n",
    "    while job.state not in completed_states:\n",
    "        time.sleep(30)\n",
    "        job = client.batches.get(name=job.name)\n",
    "        print(f\"Job state: {job.state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eda17916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5701, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "files = [f for f in os.listdir(\"./output/\") if f.startswith(\"output_prediction\")]\n",
    "\n",
    "\n",
    "\n",
    "def file_2_df(file):\n",
    "    requests = []\n",
    "    responses = []\n",
    "\n",
    "    with open(\"./output/\" + file, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = json.loads(line.strip())\n",
    "            request = line['request']['contents'][0]['parts'][0]['text']\n",
    "            response = line['response']['candidates'][0]['content']['parts'][0]['text']\n",
    "            requests.append(request)\n",
    "            responses.append(response)\n",
    "        \n",
    "    df = pd.DataFrame({'request': requests, 'response': responses})\n",
    "    return df\n",
    "\n",
    "for file in files:\n",
    "    df = pd.concat([df, file_2_df(file)], ignore_index=True)\n",
    "\n",
    "pd.set_option('display.max_colwidth',True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "855d9efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Write a consise, formal and structured title for this scientific research work, return ONLY the title:\\n\"\n",
    "test_df = pd.read_csv(\"./data/test.csv\")['abstract']\n",
    "\n",
    "# Remove the instruction prefix from df's request column\n",
    "df['abstract'] = df['request'].str.replace(instruction, '', regex=False)\n",
    "\n",
    "# Merge df with test_df based on the abstract content\n",
    "test_df = test_df.to_frame(name='abstract')\n",
    "merged_df = test_df.merge(df, on='abstract', how='left')\n",
    "\n",
    "# Drop the abstract column from the merged dataframe\n",
    "merged_df = merged_df.drop(columns=['abstract'])\n",
    "merged_df = merged_df.rename(columns={'response': 'title'})\n",
    "\n",
    "merged_df['title'] = merged_df['title'].str.strip('\\n\"')\n",
    "merged_df['title'] = merged_df['title'].str.lstrip('\"')\n",
    "\n",
    "merged_df['title'].to_csv(\"./output/gemini-2.0-flash-001-output.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
