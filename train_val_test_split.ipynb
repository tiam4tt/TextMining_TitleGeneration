{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import T5TokenizerFast\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datapath = './data/train.csv'\n",
    "# train_save='./data/subset_train.csv'\n",
    "# test_save='./data/subset_test.csv'\n",
    "# val_save='./data/subset_val.csv'\n",
    "\n",
    "datapath = 'filtered_data.csv'\n",
    "train_save='./data/train.csv'\n",
    "test_save='./data/test.csv'\n",
    "val_save='./data/val.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(datapath)\n",
    "if datapath == './data/train.csv':\n",
    "    df = df.sample(n=10000, random_state=42)\n",
    "    print(\"Sampled 10k random data points\")\n",
    "else:\n",
    "    mask = (df['abstract_tokens'].between(128, 450)) & (df['title_tokens'].between(8, 32))\n",
    "    df = df[mask]\n",
    "    df.shape\n",
    "\n",
    "    # import matplotlib.pyplot as plt\n",
    "\n",
    "    # # Plot histograms\n",
    "    # plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # plt.subplot(1, 2, 1)\n",
    "    # plt.hist(df['abstract_tokens'], bins=50, color='blue', alpha=0.7)\n",
    "    # plt.title('Distribution of Abstract Tokens')\n",
    "    # plt.xlabel('Number of Tokens')\n",
    "    # plt.ylabel('Frequency')\n",
    "\n",
    "    # plt.subplot(1, 2, 2)\n",
    "    # plt.hist(df['title_tokens'], bins=50, color='green', alpha=0.7)\n",
    "    # plt.title('Distribution of Title Tokens')\n",
    "    # plt.xlabel('Number of Tokens')\n",
    "    # plt.ylabel('Frequency')\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74488, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    df = df.drop(columns=[\"abstract_tokens\", \"title_tokens\"])\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine similarity trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    74488.000000\n",
       "mean         0.410170\n",
       "std          0.153956\n",
       "min          0.000000\n",
       "25%          0.297470\n",
       "50%          0.408061\n",
       "75%          0.519604\n",
       "max          0.903722\n",
       "Name: cosine_similarity, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(df['abstract'].tolist() + df['title'].tolist())\n",
    "\n",
    "title_vectors = tfidf_matrix[:len(df)]\n",
    "abstract_vectors = tfidf_matrix[len(df):]\n",
    "\n",
    "df[\"cosine_similarity\"] = [\n",
    "    cosine_similarity(title_vectors[i], abstract_vectors[i])[0][0]\n",
    "    for i in range(len(df))\n",
    "]\n",
    "df[\"cosine_similarity\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trim data where cosine similarity is above 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(223, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df[\"cosine_similarity\"] >= 0.8]\n",
    "df = df.drop(columns=[\"cosine_similarity\"])\n",
    "# df.to_csv(\"trimmed_filtered_data.csv\", index=False)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split_and_count(tokenized_df, min_freq=3, qcut=5):\n",
    "\n",
    "    # Flatten all tokens from titles and abstracts\n",
    "    all_tokens = (\n",
    "        tokenized_df['tokenized_title'].explode().tolist()\n",
    "        + tokenized_df['tokenized_abstract'].explode().tolist()\n",
    "    )\n",
    "\n",
    "    token_freq = Counter(all_tokens)\n",
    "\n",
    "    valid_tokens = {tok for tok, freq in token_freq.items() if freq >= min_freq}\n",
    "\n",
    "    # Calculate vocabulary for each abstract\n",
    "    tokenized_df[\"vocab\"] = tokenized_df.apply(\n",
    "            lambda row: set(\n",
    "                tok for tok in row[\"tokenized_abstract\"] + row[\"tokenized_title\"]\n",
    "                if tok in valid_tokens\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "    # Create a global vocabulary\n",
    "    global_vocab = set().union(*tokenized_df[\"vocab\"])\n",
    "\n",
    "    # Calculate overlap with global vocabulary\n",
    "    tokenized_df[\"overlap\"] = tokenized_df[\"vocab\"].apply(\n",
    "        lambda x: len(x & global_vocab) / len(global_vocab) if global_vocab else 0\n",
    "    )\n",
    "\n",
    "    # Bin overlap into categories for stratification\n",
    "    tokenized_df[\"overlap_bin\"] = pd.qcut(tokenized_df[\"overlap\"], q=qcut, labels=False)\n",
    "\n",
    "    # Perform stratified splitting\n",
    "    train_idx, temp_idx = train_test_split(\n",
    "        tokenized_df.index,\n",
    "        test_size=0.2,\n",
    "        stratify=tokenized_df[\"overlap_bin\"],\n",
    "        random_state=42,\n",
    "    )\n",
    "    val_idx, test_idx = train_test_split(\n",
    "        temp_idx,\n",
    "        test_size=0.5,\n",
    "        stratify=tokenized_df.loc[temp_idx, \"overlap_bin\"],\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    # Extract dataframes\n",
    "    train_df = tokenized_df.loc[train_idx]\n",
    "    val_df = tokenized_df.loc[val_idx]\n",
    "    test_df = tokenized_df.loc[test_idx]\n",
    "\n",
    "    # Count words in val and test that are not in train\n",
    "    train_vocab = set().union(*train_df[\"vocab\"])\n",
    "    val_vocab = set().union(*val_df[\"vocab\"])\n",
    "    test_vocab = set().union(*test_df[\"vocab\"])\n",
    "\n",
    "    val_not_train = len(val_vocab - train_vocab)\n",
    "    test_not_train = len(test_vocab - train_vocab)\n",
    "\n",
    "    return {\n",
    "        \"train_df\": df.loc[train_idx],\n",
    "        \"val_df\": df.loc[val_idx],\n",
    "        \"test_df\": df.loc[test_idx],\n",
    "        \"val-train\": val_not_train,\n",
    "        \"test-train\": test_not_train,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5TokenizerFast.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "tokenized_df = pd.DataFrame()\n",
    "tokenized_df['tokenized_title'] = df['title'].apply(lambda x: tokenizer.tokenize(x))\n",
    "tokenized_df['tokenized_abstract'] = df['abstract'].apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words that are in VALIDATION set but no in TRAIN set: 69\n",
      "Number of words that are in TEST set but not in TRAIN set: 63\n"
     ]
    }
   ],
   "source": [
    "cache = stratified_split_and_count(tokenized_df)\n",
    "print(f\"Number of words that are in VALIDATION set but no in TRAIN set: {cache['val-train']}\")\n",
    "print(f\"Number of words that are in TEST set but not in TRAIN set: {cache['test-train']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train, val and test datasets:\n",
      "Train: (178, 3) saved to ./data/train.csv\n",
      "Validation: (22, 3) saved to ./data/val.csv\n",
      "Test: (23, 3) saved to ./data/test.csv\n"
     ]
    }
   ],
   "source": [
    "cache['train_df'].to_csv(train_save, index=False)\n",
    "cache['val_df'].to_csv(val_save, index=False)\n",
    "cache['test_df'].to_csv(test_save, index=False)\n",
    "\n",
    "print(\"Size of train, val and test datasets:\")\n",
    "print(f\"Train: {cache['train_df'].shape} saved to {train_save}\")\n",
    "print(f\"Validation: {cache['val_df'].shape} saved to {val_save}\")\n",
    "print(f\"Test: {cache['test_df'].shape} saved to {test_save}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
